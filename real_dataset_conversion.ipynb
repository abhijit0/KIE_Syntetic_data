{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'all_annotations/dekra_annotations/'\n",
    "annotations_dir = f'{root_dir}/annotations/instances_default.json'\n",
    "image_dir = f'{root_dir}/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with open(annotations_dir) as f:\n",
    "#        configs = json.load(f)\n",
    "        \n",
    "#print(configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configs['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a8hik/anaconda3/envs/py3_10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at microsoft/layoutxlm-base were not used when initializing LayoutLMv2Model: ['layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from utility_functions.utilities_kie import * \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutxlm-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/layoutxlm-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def preprocess_tokens(tokens:list=None, bboxes:list=None):\n",
    "    speical_characters = set(string.punctuation)\n",
    "    tokens_preprocessed = []\n",
    "    bboxes_preprocessed = []\n",
    "        \n",
    "    for i, token in enumerate(tokens):\n",
    "        token = token.lower()\n",
    "        for char_ in speical_characters:\n",
    "            token = token.strip(char_)\n",
    "        if token not in speical_characters and token not in (' ',  '', '\\t','\\n') and len(token) > 0:\n",
    "            tokens_preprocessed.append(token)\n",
    "            bboxes_preprocessed.append(bboxes[i])\n",
    "        \n",
    "    return tokens_preprocessed, bboxes_preprocessed\n",
    "\n",
    "def load_annotation_image(image_id:int=None, configs:dict=None):\n",
    "    annotations = []\n",
    "    for annotation in configs[\"annotations\"]:\n",
    "        if annotation[\"image_id\"] == image_id:\n",
    "            annotations.append(annotation)\n",
    "    return annotations\n",
    "        \n",
    "def load_category(category_id:int=None, configs:dict=None):\n",
    "    for category in configs[\"categories\"]:\n",
    "        if category[\"id\"] == category_id:\n",
    "            return category\n",
    "\n",
    "\n",
    "def postprocess_bobx(bbox, image_dim):\n",
    "    bbox_postprocessed = []\n",
    "    \n",
    "    #for i in bbox:\n",
    "    #    if i - float(int(i)) < 0.5:\n",
    "    #        val = int(np.floor(i))\n",
    "    #        bbox_postprocessed.append(val)\n",
    "    #    else:\n",
    "    #        val = int(np.ceil(i))\n",
    "    #        bbox_postprocessed.append(val)\n",
    "    bbox_postprocessed = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]]\n",
    "    \n",
    "    bbox_postprocessed = normalize_bbox(bbox_postprocessed, width=image_dim['width'], height = image_dim['height'])\n",
    "    \n",
    "    return bbox_postprocessed\n",
    "\n",
    "\n",
    "def split_text_bboxes(bbox, text, category):\n",
    "    #print(text)\n",
    "    tokens = text.split(' ')\n",
    "    bboxes = [bbox for _ in range(len(tokens))]\n",
    "    tokens, bboxes = preprocess_tokens(tokens=tokens, bboxes=bboxes)\n",
    "    categories = [category for _ in range(len(tokens))]\n",
    "    return tokens, bboxes, categories\n",
    "\n",
    "def form_entities(token_cat_groups, tokenizer, input_ids, input_id_map):\n",
    "    entities = {'start':[], 'end':[], 'label':[], 'cat':[]}\n",
    "    #print(token_cat_groups)\n",
    "    for token_cat_group in token_cat_groups:\n",
    "        token_group = token_cat_group[0]\n",
    "        cat_group = token_cat_group[1]\n",
    "        \n",
    "        if len(cat_group) >0 and len(token_group) >0:\n",
    "            cat = cat_group[0]\n",
    "        \n",
    "            input_ids_token_group = [input_id_map[tokenized_token] for token in token_group for tokenized_token in tokenizer.tokenize(token) if tokenized_token in input_id_map.keys()]\n",
    "            indexes = find_sequnce_indices(sequence=input_ids_token_group, input_ids=input_ids)\n",
    "        \n",
    "            start_idx = indexes[0][0]\n",
    "            end_idx = indexes[0][-1]\n",
    "        \n",
    "            entities['start'].append(start_idx)  \n",
    "            entities['end'] .append(end_idx)\n",
    "            if '_key' in cat:\n",
    "                entities['label'].append('QUESTION')\n",
    "            elif '_value' in cat:\n",
    "                entities['label'].append('ANSWER')\n",
    "            else:\n",
    "                entities['label'].append('OTHER')\n",
    "            entities['cat'].append(cat)\n",
    "    return entities\n",
    "    \n",
    "        \n",
    "        \n",
    "           \n",
    "\n",
    "def label_input_ids(annotation, category, bbox):\n",
    "    label_vals = {'O' : 0, 'B-QUESTION' : 1, 'B-ANSWER' : 2, 'B-HEADER' : 3, 'I-ANSWER' : 4, 'I-QUESTION' : 5, 'I-HEADER' : 6}\n",
    "    \n",
    "    annotation_attributes = annotation['attributes']\n",
    "    #bbox = postprocess_bobx(annotation['bbox'])\n",
    "    text = annotation_attributes['content']\n",
    "    tokens, _, _ = split_text_bboxes(bbox, text, category)\n",
    "    labels = []\n",
    "        \n",
    "    for i in range(len(tokens)):\n",
    "        if i==0:\n",
    "            if 'key' in category:\n",
    "                labels.append(label_vals['B-QUESTION'])\n",
    "            elif 'value' in category:\n",
    "                labels.append(label_vals['B-ANSWER'])\n",
    "            else:\n",
    "                labels.append(label_vals['O'])\n",
    "        else:\n",
    "            if 'key' in category:\n",
    "                labels.append(label_vals['I-QUESTION'])\n",
    "            elif 'value' in category:\n",
    "                labels.append(label_vals['I-ANSWER'])\n",
    "            else:\n",
    "                labels.append(label_vals['O'])\n",
    "        \n",
    "    return labels\n",
    "\n",
    "def form_relations(entities, input_ids):\n",
    "    entity_names_cat = {'entity_names':[], 'cat':[]}\n",
    "    relations={'head':[], 'tail':[]}\n",
    "    for i , (start, end) in enumerate(zip(entities['start'], entities['end'])):\n",
    "        #print(f'{i}, {tokenizer.decode(input_ids[start:end])} : {entities[\"label\"][i]} {entities[\"cat\"][i]}')\n",
    "        #if entities[\"label\"][i].lower() != 'other':\n",
    "        entity_names_cat['entity_names'].append(tokenizer.decode(input_ids[start:end]))\n",
    "        entity_names_cat['cat'].append(entities['cat'][i])\n",
    " \n",
    "    for i, entity in enumerate(entity_names_cat['entity_names']):\n",
    "        cat = entity_names_cat['cat'][i]\n",
    "        if '_key' in cat.lower():         \n",
    "            j = entity_names_cat['cat'].index(f'{cat[:-4:]}_value') if f'{cat[:-4:]}_value' in entity_names_cat['cat'] else None\n",
    "            if j is not None:\n",
    "                head = i\n",
    "                tail = j\n",
    "                relations['head'].append(head)\n",
    "                relations['tail'].append(tail)\n",
    "            \n",
    "                \n",
    "            \n",
    "        \n",
    "    return entity_names_cat, relations\n",
    "        \n",
    "def dump_pickle(file_name:str=None, collection:object=None):\n",
    "        file_name = os.path.join(os.getcwd(), file_name)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(collection, f)\n",
    "            \n",
    "def combine_annotations(image_id:int =None, configs=None, tokenizer=None, model =None, image_dim:dict=None):\n",
    "    annotations = load_annotation_image(image_id=image_id, configs=configs)\n",
    "    filtered_annotations = {'image_id':image_id, 'bboxes':[], 'categories':[], 'text':[], 'labels' :[], 'input_ids':[], 'entities':[], 'relations':[], 'input_id_map':{}, 'token_cat_groups':[]}\n",
    "    for annotation in annotations:\n",
    "        bbox = postprocess_bobx(annotation['bbox'], image_dim)\n",
    "        category = load_category(category_id=annotation['category_id'], configs=configs)[\"name\"]\n",
    "        \n",
    "        annotation_attributes = annotation['attributes']\n",
    "        if 'type' in annotation_attributes.keys():\n",
    "            type = annotation_attributes['type']\n",
    "            filtered_annotations['categories'].append(f'{category}_{type}')\n",
    "            category = f'{category}_{type}'\n",
    "        \n",
    "        text = annotation_attributes['content']\n",
    "        tokens, bboxes, categories = split_text_bboxes(bbox, text, category)\n",
    "        token_cat_group = (tokens, categories)\n",
    "        filtered_annotations['token_cat_groups'].append(token_cat_group)\n",
    "        labels = label_input_ids(annotation, category, bbox)\n",
    "        filtered_annotations['bboxes'].extend(bboxes)\n",
    "        filtered_annotations['text'].extend(tokens)\n",
    "        filtered_annotations['categories'].extend(categories)\n",
    "        filtered_annotations['labels'].extend(labels)\n",
    "    \n",
    "    tokens, bbox = preprocess_tokens(tokens=filtered_annotations['text'], bboxes=filtered_annotations['bboxes'])\n",
    "    tokenizer, model = add_tokens_tokenizer(tokens = tokens, tokenizer = tokenizer, model = model)\n",
    "    input_ids, bboxes, input_id_map = encode_tokens(tokens=tokens, bboxes=bbox, tokenizer=tokenizer)\n",
    "    filtered_annotations['input_id_map'] = input_id_map\n",
    "    filtered_annotations['input_ids'].extend(input_ids)\n",
    "    \n",
    "    #print(input_ids)\n",
    "    #print(input_id_map)\n",
    "    \n",
    "        \n",
    "    return filtered_annotations\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"images\"\n",
    "images_resized_dir = \"images_resized\"\n",
    "bbox_dir=\"bbox\"\n",
    "input_ids_dir = \"input_ids\"\n",
    "labels_dir = \"labels\"\n",
    "entities_dir = \"entities\"\n",
    "relations_dir = \"relations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impl 1 dataset generation funsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def create_directory_structure(dataset_dir:str, sub_dir:list, overwrite:bool):\\n    if overwrite and os.path.exists(dataset_dir):\\n        shutil.rmtree(dataset_dir)\\n        \\n    os.mkdir(dataset_dir)\\n    \\n    sub_dirs=[\\'train\\', \\'validation\\']\\n    for sub_dir in sub_dirs:\\n        inner_dir =f\\'{dataset_dir}/{sub_dir}\\' \\n        os.mkdir(inner_dir)\\n        os.mkdir(f\\'{inner_dir}/{images_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{images_resized_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{bbox_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{labels_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{entities_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{relations_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{input_ids_dir}\\')\\n\\ndef load_dataset_funsd(annotations_config:str=None, image_dir:str=None, dataset_dir:str= \\'dataset_real\\', tokenizer=None, model=None, train_test_split:float=0.05, overwrite :bool=True):\\n    sub_dirs = [\\'train\\', \\'validation\\']\\n    create_directory_structure(dataset_dir, sub_dirs, True)\\n \\n    with open(annotations_config) as f:\\n        configs = json.load(f)\\n    label_vals = {\\'O\\' : 0, \\'B-QUESTION\\' : 1, \\'B-ANSWER\\' : 2, \\'B-HEADER\\' : 3, \\'I-ANSWER\\' : 4, \\'I-QUESTION\\' : 5, \\'I-HEADER\\' : 6}\\n    label_vals_reversed = {val:key for (key,val) in label_vals.items()} \\n    image_ids = []    \\n    indices_to_keep = []\\n    print(\\'populating valid image indices\\')\\n    for i,image_obj in enumerate(configs[\\'images\\']):\\n        if i>=0:\\n            image_ids.append(image_obj[\\'id\\'])\\n            \\n            image_path = f\\'{image_dir}/{image_obj[\"file_name\"]}\\'\\n            img_height = image_obj[\"height\"]\\n            img_width = image_obj[\"width\"]\\n        \\n            combined_annotations = combine_annotations(image_id=image_obj[\\'id\\'], configs=configs, tokenizer=tokenizer, model=model, image_dim={\\'width\\':img_width, \\'height\\':img_height})\\n            entities = form_entities(combined_annotations[\\'token_cat_groups\\'], tokenizer, combined_annotations[\\'input_ids\\'], combined_annotations[\\'input_id_map\\'])\\n            combined_annotations[\\'entities\\'] = entities\\n            entity_names_cat, relations = form_relations(entities, combined_annotations[\\'input_ids\\'])\\n            if len(relations[\\'head\\']) > 0:\\n                indices_to_keep.append(i)\\n            print(f\\'{i} {image_obj[\"file_name\"]}\\')\\n            for i in range(len(relations[\\'head\\'])):\\n                print(f\\'question : {entity_names_cat[\"entity_names\"][relations[\"head\"][i]]}, Answer :  {entity_names_cat[\"entity_names\"][relations[\"tail\"][i]]}, start: {relations[\"head\"][i]}, end: {relations[\"tail\"][i]}\\')\\n            print(\\'------------\\')\\n            #for i in range(len(relations[\\'head\\'])):\\n            #    print(f\\'question : {entity_names_cat[\"entity_names\"][relations[\"head\"][i]]}, Answer :  {entity_names_cat[\"entity_names\"][relations[\"tail\"][i]]}, start: {relations[\"head\"][i]}, end: {relations[\"tail\"][i]}\\')\\n        \\n            #print(\\'-------------------------------------------------\\')\\n    print(indices_to_keep)\\n    print(\\'Generating dataset\\')\\n    validation_percentage = int(train_test_split * len(indices_to_keep))\\n    \\n    validation_indices = random.choices(indices_to_keep, k= validation_percentage)\\n    for i,image_obj in enumerate(configs[\\'images\\']):\\n        if i in indices_to_keep:\\n            image_ids.append(image_obj[\\'id\\'])    \\n            image_path = f\\'{image_dir}/{image_obj[\"file_name\"]}\\'\\n            img_height = image_obj[\"height\"]\\n            img_width = image_obj[\"width\"]\\n        \\n            combined_annotations = combine_annotations(image_id=image_obj[\\'id\\'], configs=configs, tokenizer=tokenizer, model=model, image_dim={\\'width\\':img_width, \\'height\\':img_height})\\n            entities = form_entities(combined_annotations[\\'token_cat_groups\\'], tokenizer, combined_annotations[\\'input_ids\\'], combined_annotations[\\'input_id_map\\'])\\n            combined_annotations[\\'entities\\'] = entities\\n            entity_names_cat, relations = form_relations(entities, combined_annotations[\\'input_ids\\'])\\n            \\n            image = cv2.imread(image_path)\\n            image_resized = cv2.resize(image, (224,224))\\n            bbox = combined_annotations[\\'bboxes\\']\\n            input_ids = combined_annotations[\\'input_ids\\']\\n            labels = combined_annotations[\\'labels\\']\\n        \\n            if i not in validation_indices:\\n                cv2.imwrite(f\\'{dataset_dir}/train/{images_dir}/image_{indices_to_keep.index(i)}.jpeg\\', image)\\n                cv2.imwrite(f\\'{dataset_dir}/train/{images_resized_dir}/image_resized_{indices_to_keep.index(i)}.jpeg\\', image_resized)\\n                bbox_path = f\\'{dataset_dir}/train/{bbox_dir}/bbox_{indices_to_keep.index(i)}.p\\'\\n                labels_path = f\\'{dataset_dir}/train/{labels_dir}/labels_{indices_to_keep.index(i)}.p\\'\\n                entities_path = f\\'{dataset_dir}/train/{entities_dir}/entities_{indices_to_keep.index(i)}.p\\'\\n                input_ids_path = f\\'{dataset_dir}/train/{input_ids_dir}/input_ids_{indices_to_keep.index(i)}.p\\'\\n                realations_path = f\\'{dataset_dir}/train/{relations_dir}/relations_{indices_to_keep.index(i)}.p\\'\\n            \\n                dump_pickle(file_name=bbox_path, collection=bbox)\\n                dump_pickle(file_name=labels_path, collection=labels)\\n                dump_pickle(file_name=entities_path, collection=entities)\\n                dump_pickle(file_name=input_ids_path, collection=input_ids)\\n                dump_pickle(file_name=realations_path, collection=relations)\\n            else:\\n                cv2.imwrite(f\\'{dataset_dir}/validation/{images_dir}/image_{indices_to_keep.index(i)}.jpeg\\', image)\\n                cv2.imwrite(f\\'{dataset_dir}/validation/{images_resized_dir}/image_resized_{indices_to_keep.index(i)}.jpeg\\', image_resized)\\n                bbox_path = f\\'{dataset_dir}/validation/{bbox_dir}/bbox_{indices_to_keep.index(i)}.p\\'\\n                labels_path = f\\'{dataset_dir}/validation/{labels_dir}/labels_{indices_to_keep.index(i)}.p\\'\\n                entities_path = f\\'{dataset_dir}/validation/{entities_dir}/entities_{indices_to_keep.index(i)}.p\\'\\n                input_ids_path = f\\'{dataset_dir}/validation/{input_ids_dir}/input_ids_{indices_to_keep.index(i)}.p\\'\\n                realations_path = f\\'{dataset_dir}/validation/{relations_dir}/relations_{indices_to_keep.index(i)}.p\\'\\n            \\n                dump_pickle(file_name=bbox_path, collection=bbox)\\n                dump_pickle(file_name=labels_path, collection=labels)\\n                dump_pickle(file_name=entities_path, collection=entities)\\n                dump_pickle(file_name=input_ids_path, collection=input_ids)\\n                dump_pickle(file_name=realations_path, collection=relations)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "'''def create_directory_structure(dataset_dir:str, sub_dir:list, overwrite:bool):\n",
    "    if overwrite and os.path.exists(dataset_dir):\n",
    "        shutil.rmtree(dataset_dir)\n",
    "        \n",
    "    os.mkdir(dataset_dir)\n",
    "    \n",
    "    sub_dirs=['train', 'validation']\n",
    "    for sub_dir in sub_dirs:\n",
    "        inner_dir =f'{dataset_dir}/{sub_dir}' \n",
    "        os.mkdir(inner_dir)\n",
    "        os.mkdir(f'{inner_dir}/{images_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{images_resized_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{bbox_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{labels_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{entities_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{relations_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{input_ids_dir}')\n",
    "\n",
    "def load_dataset_funsd(annotations_config:str=None, image_dir:str=None, dataset_dir:str= 'dataset_real', tokenizer=None, model=None, train_test_split:float=0.05, overwrite :bool=True):\n",
    "    sub_dirs = ['train', 'validation']\n",
    "    create_directory_structure(dataset_dir, sub_dirs, True)\n",
    " \n",
    "    with open(annotations_config) as f:\n",
    "        configs = json.load(f)\n",
    "    label_vals = {'O' : 0, 'B-QUESTION' : 1, 'B-ANSWER' : 2, 'B-HEADER' : 3, 'I-ANSWER' : 4, 'I-QUESTION' : 5, 'I-HEADER' : 6}\n",
    "    label_vals_reversed = {val:key for (key,val) in label_vals.items()} \n",
    "    image_ids = []    \n",
    "    indices_to_keep = []\n",
    "    print('populating valid image indices')\n",
    "    for i,image_obj in enumerate(configs['images']):\n",
    "        if i>=0:\n",
    "            image_ids.append(image_obj['id'])\n",
    "            \n",
    "            image_path = f'{image_dir}/{image_obj[\"file_name\"]}'\n",
    "            img_height = image_obj[\"height\"]\n",
    "            img_width = image_obj[\"width\"]\n",
    "        \n",
    "            combined_annotations = combine_annotations(image_id=image_obj['id'], configs=configs, tokenizer=tokenizer, model=model, image_dim={'width':img_width, 'height':img_height})\n",
    "            entities = form_entities(combined_annotations['token_cat_groups'], tokenizer, combined_annotations['input_ids'], combined_annotations['input_id_map'])\n",
    "            combined_annotations['entities'] = entities\n",
    "            entity_names_cat, relations = form_relations(entities, combined_annotations['input_ids'])\n",
    "            if len(relations['head']) > 0:\n",
    "                indices_to_keep.append(i)\n",
    "            print(f'{i} {image_obj[\"file_name\"]}')\n",
    "            for i in range(len(relations['head'])):\n",
    "                print(f'question : {entity_names_cat[\"entity_names\"][relations[\"head\"][i]]}, Answer :  {entity_names_cat[\"entity_names\"][relations[\"tail\"][i]]}, start: {relations[\"head\"][i]}, end: {relations[\"tail\"][i]}')\n",
    "            print('------------')\n",
    "            #for i in range(len(relations['head'])):\n",
    "            #    print(f'question : {entity_names_cat[\"entity_names\"][relations[\"head\"][i]]}, Answer :  {entity_names_cat[\"entity_names\"][relations[\"tail\"][i]]}, start: {relations[\"head\"][i]}, end: {relations[\"tail\"][i]}')\n",
    "        \n",
    "            #print('-------------------------------------------------')\n",
    "    print(indices_to_keep)\n",
    "    print('Generating dataset')\n",
    "    validation_percentage = int(train_test_split * len(indices_to_keep))\n",
    "    \n",
    "    validation_indices = random.choices(indices_to_keep, k= validation_percentage)\n",
    "    for i,image_obj in enumerate(configs['images']):\n",
    "        if i in indices_to_keep:\n",
    "            image_ids.append(image_obj['id'])    \n",
    "            image_path = f'{image_dir}/{image_obj[\"file_name\"]}'\n",
    "            img_height = image_obj[\"height\"]\n",
    "            img_width = image_obj[\"width\"]\n",
    "        \n",
    "            combined_annotations = combine_annotations(image_id=image_obj['id'], configs=configs, tokenizer=tokenizer, model=model, image_dim={'width':img_width, 'height':img_height})\n",
    "            entities = form_entities(combined_annotations['token_cat_groups'], tokenizer, combined_annotations['input_ids'], combined_annotations['input_id_map'])\n",
    "            combined_annotations['entities'] = entities\n",
    "            entity_names_cat, relations = form_relations(entities, combined_annotations['input_ids'])\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            image_resized = cv2.resize(image, (224,224))\n",
    "            bbox = combined_annotations['bboxes']\n",
    "            input_ids = combined_annotations['input_ids']\n",
    "            labels = combined_annotations['labels']\n",
    "        \n",
    "            if i not in validation_indices:\n",
    "                cv2.imwrite(f'{dataset_dir}/train/{images_dir}/image_{indices_to_keep.index(i)}.jpeg', image)\n",
    "                cv2.imwrite(f'{dataset_dir}/train/{images_resized_dir}/image_resized_{indices_to_keep.index(i)}.jpeg', image_resized)\n",
    "                bbox_path = f'{dataset_dir}/train/{bbox_dir}/bbox_{indices_to_keep.index(i)}.p'\n",
    "                labels_path = f'{dataset_dir}/train/{labels_dir}/labels_{indices_to_keep.index(i)}.p'\n",
    "                entities_path = f'{dataset_dir}/train/{entities_dir}/entities_{indices_to_keep.index(i)}.p'\n",
    "                input_ids_path = f'{dataset_dir}/train/{input_ids_dir}/input_ids_{indices_to_keep.index(i)}.p'\n",
    "                realations_path = f'{dataset_dir}/train/{relations_dir}/relations_{indices_to_keep.index(i)}.p'\n",
    "            \n",
    "                dump_pickle(file_name=bbox_path, collection=bbox)\n",
    "                dump_pickle(file_name=labels_path, collection=labels)\n",
    "                dump_pickle(file_name=entities_path, collection=entities)\n",
    "                dump_pickle(file_name=input_ids_path, collection=input_ids)\n",
    "                dump_pickle(file_name=realations_path, collection=relations)\n",
    "            else:\n",
    "                cv2.imwrite(f'{dataset_dir}/validation/{images_dir}/image_{indices_to_keep.index(i)}.jpeg', image)\n",
    "                cv2.imwrite(f'{dataset_dir}/validation/{images_resized_dir}/image_resized_{indices_to_keep.index(i)}.jpeg', image_resized)\n",
    "                bbox_path = f'{dataset_dir}/validation/{bbox_dir}/bbox_{indices_to_keep.index(i)}.p'\n",
    "                labels_path = f'{dataset_dir}/validation/{labels_dir}/labels_{indices_to_keep.index(i)}.p'\n",
    "                entities_path = f'{dataset_dir}/validation/{entities_dir}/entities_{indices_to_keep.index(i)}.p'\n",
    "                input_ids_path = f'{dataset_dir}/validation/{input_ids_dir}/input_ids_{indices_to_keep.index(i)}.p'\n",
    "                realations_path = f'{dataset_dir}/validation/{relations_dir}/relations_{indices_to_keep.index(i)}.p'\n",
    "            \n",
    "                dump_pickle(file_name=bbox_path, collection=bbox)\n",
    "                dump_pickle(file_name=labels_path, collection=labels)\n",
    "                dump_pickle(file_name=entities_path, collection=entities)\n",
    "                dump_pickle(file_name=input_ids_path, collection=input_ids)\n",
    "                dump_pickle(file_name=realations_path, collection=relations)'''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "         \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impl 2 dataset generation funsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def create_directory_structure(dataset_dir:str, sub_dir:list, overwrite:bool):\n",
    "    if overwrite and os.path.exists(dataset_dir):\n",
    "        shutil.rmtree(dataset_dir)\n",
    "        \n",
    "    os.mkdir(dataset_dir)\n",
    "    \n",
    "    sub_dirs=['train', 'validation']\n",
    "    for sub_dir in sub_dirs:\n",
    "        inner_dir =f'{dataset_dir}/{sub_dir}' \n",
    "        os.mkdir(inner_dir)\n",
    "        os.mkdir(f'{inner_dir}/{images_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{images_resized_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{bbox_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{labels_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{entities_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{relations_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{input_ids_dir}')\n",
    "\n",
    "def get_file_index_map(path:str=None):\n",
    "        paths = os.listdir(path)\n",
    "        sorted_paths = sorted(paths)\n",
    "        idx_map = {i:sorted_paths[i] for i in range(len(sorted_paths))}\n",
    "        return idx_map\n",
    "\n",
    "def load_dataset_funsd(annotations_configs:dict=None, image_dirs:dict=None, dataset_dir:str= 'dataset_real', tokenizer=None, model=None, train_test_split:float=0.05, overwrite :bool=True, dataset='all'):\n",
    "    sub_dirs = ['train', 'validation']\n",
    "    create_directory_structure(dataset_dir, sub_dirs, True)\n",
    "    configs_list = []\n",
    "    image_dirs_list = []\n",
    "    if dataset=='all':\n",
    "        for key in annotations_configs.keys():\n",
    "            configs_list.append(annotations_configs[key])\n",
    "            image_dirs_list.append(image_dirs[key])\n",
    "    else: \n",
    "        configs_list.append(annotations_configs[dataset])\n",
    "        image_dirs_list.append(image_dirs[dataset])\n",
    "        \n",
    "    count = 0\n",
    "    for annotations_config, image_dir in zip(configs_list, image_dirs_list):\n",
    "        record_count = 0\n",
    "        with open(annotations_config) as f:\n",
    "            configs = json.load(f)\n",
    "        label_vals = {'O' : 0, 'B-QUESTION' : 1, 'B-ANSWER' : 2, 'B-HEADER' : 3, 'I-ANSWER' : 4, 'I-QUESTION' : 5, 'I-HEADER' : 6}\n",
    "        label_vals_reversed = {val:key for (key,val) in label_vals.items()} \n",
    "        image_ids = []    \n",
    "        indices_to_keep = []\n",
    "        print('populating valid image indices')\n",
    "        for i,image_obj in enumerate(configs['images']):\n",
    "            if i>=0:\n",
    "                image_ids.append(image_obj['id'])\n",
    "            \n",
    "                image_path = f'{image_dir}/{image_obj[\"file_name\"]}'\n",
    "                img_height = image_obj[\"height\"]\n",
    "                img_width = image_obj[\"width\"]\n",
    "        \n",
    "                combined_annotations = combine_annotations(image_id=image_obj['id'], configs=configs, tokenizer=tokenizer, model=model, image_dim={'width':img_width, 'height':img_height})\n",
    "                entities = form_entities(combined_annotations['token_cat_groups'], tokenizer, combined_annotations['input_ids'], combined_annotations['input_id_map'])\n",
    "                combined_annotations['entities'] = entities\n",
    "                entity_names_cat, relations = form_relations(entities, combined_annotations['input_ids'])\n",
    "                if len(relations['head']) > 0:\n",
    "                    #indices_to_keep.append(i)\n",
    "                    #print(f'{i} {image_obj[\"file_name\"]}')\n",
    "                    #for i in range(len(relations['head'])):\n",
    "                    #    print(f'question : {entity_names_cat[\"entity_names\"][relations[\"head\"][i]]}, Answer :  {entity_names_cat[\"entity_names\"][relations[\"tail\"][i]]}, start: {relations[\"head\"][i]}, end: {relations[\"tail\"][i]}')\n",
    "                    #print('------------')\n",
    "                    image = cv2.imread(image_path)\n",
    "                    image_resized = cv2.resize(image, (224,224))\n",
    "                    bbox = combined_annotations['bboxes']\n",
    "                    input_ids = combined_annotations['input_ids']\n",
    "                    labels = combined_annotations['labels']\n",
    "                \n",
    "                    cv2.imwrite(f'{dataset_dir}/train/{images_dir}/image_{i+count}.jpeg', image)\n",
    "                    cv2.imwrite(f'{dataset_dir}/train/{images_resized_dir}/image_resized_{i+count}.jpeg', image_resized)\n",
    "                    bbox_path = f'{dataset_dir}/train/{bbox_dir}/bbox_{i+count}.p'\n",
    "                    labels_path = f'{dataset_dir}/train/{labels_dir}/labels_{i+count}.p'\n",
    "                    entities_path = f'{dataset_dir}/train/{entities_dir}/entities_{i+count}.p'\n",
    "                    input_ids_path = f'{dataset_dir}/train/{input_ids_dir}/input_ids_{i+count}.p'\n",
    "                    realations_path = f'{dataset_dir}/train/{relations_dir}/relations_{i+count}.p'\n",
    "            \n",
    "                    dump_pickle(file_name=bbox_path, collection=bbox)\n",
    "                    dump_pickle(file_name=labels_path, collection=labels)\n",
    "                    dump_pickle(file_name=entities_path, collection=entities)\n",
    "                    dump_pickle(file_name=input_ids_path, collection=input_ids)\n",
    "                    dump_pickle(file_name=realations_path, collection=relations)\n",
    "                    record_count+=1\n",
    "        count+=record_count\n",
    "\n",
    "def split_data(train_test_split:float=None, dataset_dir:str=None):\n",
    "    file_indices = [i for i in range(len(os.listdir(f'{dataset_dir}/train/images')))]\n",
    "    validation_percentage = int(train_test_split * len(file_indices))\n",
    "    validation_indices = random.choices(file_indices, k= validation_percentage)\n",
    "    \n",
    "    idx_map_image = get_file_index_map(path = f'{dataset_dir}/train/images')\n",
    "    idx_map_image_resized = get_file_index_map(path = f'{dataset_dir}/train/images_resized')\n",
    "    idx_map_bbox = get_file_index_map(path = f'{dataset_dir}/train/bbox')\n",
    "    idx_map_labels = get_file_index_map(path = f'{dataset_dir}/train/labels')\n",
    "    idx_map_entities = get_file_index_map(path = f'{dataset_dir}/train/entities')\n",
    "    idx_map_realations = get_file_index_map(path = f'{dataset_dir}/train/relations')\n",
    "    idx_map_input_ids = get_file_index_map(path = f'{dataset_dir}/train/input_ids')\n",
    "    \n",
    "    print(validation_indices)\n",
    "    for i in validation_indices:\n",
    "        shutil.move(f'{dataset_dir}/train/images/{idx_map_image[i]}', f'{dataset_dir}/validation/images/{idx_map_image[i]}')\n",
    "        shutil.move(f'{dataset_dir}/train/images_resized/{idx_map_image_resized[i]}', f'{dataset_dir}/validation/images_resized/{idx_map_image_resized[i]}')\n",
    "        shutil.move(f'{dataset_dir}/train/bbox/{idx_map_bbox[i]}', f'{dataset_dir}/validation/bbox/{idx_map_bbox[i]}')\n",
    "        shutil.move(f'{dataset_dir}/train/input_ids/{idx_map_input_ids[i]}', f'{dataset_dir}/validation/input_ids/{idx_map_input_ids[i]}')\n",
    "        shutil.move(f'{dataset_dir}/train/labels/{idx_map_labels[i]}', f'{dataset_dir}/validation/labels/{idx_map_labels[i]}')\n",
    "        shutil.move(f'{dataset_dir}/train/entities/{idx_map_entities[i]}', f'{dataset_dir}/validation/entities/{idx_map_entities[i]}')\n",
    "        shutil.move(f'{dataset_dir}/train/relations/{idx_map_realations[i]}', f'{dataset_dir}/validation/relations/{idx_map_realations[i]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impl3 dataset generation funsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def create_directory_structure(dataset_dir:str, sub_dir:list, overwrite:bool):\\n    if overwrite and os.path.exists(dataset_dir):\\n        shutil.rmtree(dataset_dir)\\n        \\n    os.mkdir(dataset_dir)\\n    \\n    sub_dirs=[\\'train\\', \\'validation\\']\\n    for sub_dir in sub_dirs:\\n        inner_dir =f\\'{dataset_dir}/{sub_dir}\\' \\n        os.mkdir(inner_dir)\\n        os.mkdir(f\\'{inner_dir}/{images_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{images_resized_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{bbox_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{labels_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{entities_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{relations_dir}\\')\\n        os.mkdir(f\\'{inner_dir}/{input_ids_dir}\\')\\n\\ndef load_dataset_funsd(annotations_configs:dict=None, image_dirs:dict=None, dataset_dir:str= \\'dataset_real\\', tokenizer=None, model=None, train_test_split:float=0.05, overwrite :bool=True, dataset=\\'all\\'):\\n    sub_dirs = [\\'train\\', \\'validation\\']\\n    create_directory_structure(dataset_dir, sub_dirs, True)\\n    \\n    configs_list = []\\n    image_dirs_list = []\\n    if dataset==\\'all\\':\\n        for key in annotations_configs.keys():\\n            configs_list.append(annotations_configs[key])\\n            image_dirs_list.append(image_dirs[key])\\n    else: \\n        configs_list.append(annotations_configs[dataset])\\n        image_dirs_list.append(image_dirs[dataset])\\n    \\n    print(image_dirs_list)\\n    print(configs_list)\\n    count = 0\\n    for image_dir,annotations_config in zip(image_dirs_list, configs_list):\\n        with open(annotations_config) as f:\\n            configs = json.load(f)\\n        label_vals = {\\'O\\' : 0, \\'B-QUESTION\\' : 1, \\'B-ANSWER\\' : 2, \\'B-HEADER\\' : 3, \\'I-ANSWER\\' : 4, \\'I-QUESTION\\' : 5, \\'I-HEADER\\' : 6}\\n        label_vals_reversed = {val:key for (key,val) in label_vals.items()} \\n        image_ids = []    \\n        indices_to_keep = []\\n        print(f\\'populating valid image indices for {annotations_config}\\')\\n        for i,image_obj in enumerate(configs[\\'images\\']):\\n            image_ids.append(image_obj[\\'id\\'])\\n            \\n            image_path = f\\'{image_dir}/{image_obj[\"file_name\"]}\\'\\n            img_height = image_obj[\"height\"]\\n            img_width = image_obj[\"width\"]\\n        \\n            combined_annotations = combine_annotations(image_id=image_obj[\\'id\\'], configs=configs, tokenizer=tokenizer, model=model, image_dim={\\'width\\':img_width, \\'height\\':img_height})\\n            entities = form_entities(combined_annotations[\\'token_cat_groups\\'], tokenizer, combined_annotations[\\'input_ids\\'], combined_annotations[\\'input_id_map\\'])\\n            combined_annotations[\\'entities\\'] = entities\\n            _, relations = form_relations(entities, combined_annotations[\\'input_ids\\'])\\n            if len(relations[\\'head\\']) > 0:\\n                indices_to_keep.append(i)\\n            \\n        \\n        print(f\\'Generating dataset {annotations_config}\\')\\n        validation_percentage = int(train_test_split * len(indices_to_keep))\\n    \\n        validation_indices = random.choices(indices_to_keep, k= validation_percentage)\\n        for i,image_obj in enumerate(configs[\\'images\\']):\\n            if i in indices_to_keep:\\n                image_ids.append(image_obj[\\'id\\'])    \\n                image_path = f\\'{image_dir}/{image_obj[\"file_name\"]}\\'\\n                img_height = image_obj[\"height\"]\\n                img_width = image_obj[\"width\"]\\n        \\n                combined_annotations = combine_annotations(image_id=image_obj[\\'id\\'], configs=configs, tokenizer=tokenizer, model=model, image_dim={\\'width\\':img_width, \\'height\\':img_height})\\n                entities = form_entities(combined_annotations[\\'token_cat_groups\\'], tokenizer, combined_annotations[\\'input_ids\\'], combined_annotations[\\'input_id_map\\'])\\n                combined_annotations[\\'entities\\'] = entities\\n                _, relations = form_relations(entities, combined_annotations[\\'input_ids\\'])\\n\\n                image = cv2.imread(image_path)\\n                image_resized = cv2.resize(image, (224,224))\\n                bbox = combined_annotations[\\'bboxes\\']\\n                input_ids = combined_annotations[\\'input_ids\\']\\n                labels = combined_annotations[\\'labels\\']\\n        \\n                if i not in validation_indices:\\n                    cv2.imwrite(f\\'{dataset_dir}/train/{images_dir}/image_{indices_to_keep.index(i) + count}.jpeg\\', image)\\n                    cv2.imwrite(f\\'{dataset_dir}/train/{images_resized_dir}/image_resized_{indices_to_keep.index(i)+count}.jpeg\\', image_resized)\\n                    bbox_path = f\\'{dataset_dir}/train/{bbox_dir}/bbox_{indices_to_keep.index(i)+count}.p\\'\\n                    labels_path = f\\'{dataset_dir}/train/{labels_dir}/labels_{indices_to_keep.index(i)+count}.p\\'\\n                    entities_path = f\\'{dataset_dir}/train/{entities_dir}/entities_{indices_to_keep.index(i)+count}.p\\'\\n                    input_ids_path = f\\'{dataset_dir}/train/{input_ids_dir}/input_ids_{indices_to_keep.index(i)+count}.p\\'\\n                    realations_path = f\\'{dataset_dir}/train/{relations_dir}/relations_{indices_to_keep.index(i)+count}.p\\'\\n            \\n                    dump_pickle(file_name=bbox_path, collection=bbox)\\n                    dump_pickle(file_name=labels_path, collection=labels)\\n                    dump_pickle(file_name=entities_path, collection=entities)\\n                    dump_pickle(file_name=input_ids_path, collection=input_ids)\\n                    dump_pickle(file_name=realations_path, collection=relations)\\n                else:\\n                    cv2.imwrite(f\\'{dataset_dir}/validation/{images_dir}/image_{indices_to_keep.index(i)+count}.jpeg\\', image)\\n                    cv2.imwrite(f\\'{dataset_dir}/validation/{images_resized_dir}/image_resized_{indices_to_keep.index(i)+count}.jpeg\\', image_resized)\\n                    bbox_path = f\\'{dataset_dir}/validation/{bbox_dir}/bbox_{indices_to_keep.index(i)+count}.p\\'\\n                    labels_path = f\\'{dataset_dir}/validation/{labels_dir}/labels_{indices_to_keep.index(i)+count}.p\\'\\n                    entities_path = f\\'{dataset_dir}/validation/{entities_dir}/entities_{indices_to_keep.index(i)+count}.p\\'\\n                    input_ids_path = f\\'{dataset_dir}/validation/{input_ids_dir}/input_ids_{indices_to_keep.index(i)+count}.p\\'\\n                    realations_path = f\\'{dataset_dir}/validation/{relations_dir}/relations_{indices_to_keep.index(i)+count}.p\\'\\n            \\n                    dump_pickle(file_name=bbox_path, collection=bbox)\\n                    dump_pickle(file_name=labels_path, collection=labels)\\n                    dump_pickle(file_name=entities_path, collection=entities)\\n                    dump_pickle(file_name=input_ids_path, collection=input_ids)\\n                    dump_pickle(file_name=realations_path, collection=relations)\\n        count += len(indices_to_keep)\\n        print(\\'-------\\')'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "'''def create_directory_structure(dataset_dir:str, sub_dir:list, overwrite:bool):\n",
    "    if overwrite and os.path.exists(dataset_dir):\n",
    "        shutil.rmtree(dataset_dir)\n",
    "        \n",
    "    os.mkdir(dataset_dir)\n",
    "    \n",
    "    sub_dirs=['train', 'validation']\n",
    "    for sub_dir in sub_dirs:\n",
    "        inner_dir =f'{dataset_dir}/{sub_dir}' \n",
    "        os.mkdir(inner_dir)\n",
    "        os.mkdir(f'{inner_dir}/{images_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{images_resized_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{bbox_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{labels_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{entities_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{relations_dir}')\n",
    "        os.mkdir(f'{inner_dir}/{input_ids_dir}')\n",
    "\n",
    "def load_dataset_funsd(annotations_configs:dict=None, image_dirs:dict=None, dataset_dir:str= 'dataset_real', tokenizer=None, model=None, train_test_split:float=0.05, overwrite :bool=True, dataset='all'):\n",
    "    sub_dirs = ['train', 'validation']\n",
    "    create_directory_structure(dataset_dir, sub_dirs, True)\n",
    "    \n",
    "    configs_list = []\n",
    "    image_dirs_list = []\n",
    "    if dataset=='all':\n",
    "        for key in annotations_configs.keys():\n",
    "            configs_list.append(annotations_configs[key])\n",
    "            image_dirs_list.append(image_dirs[key])\n",
    "    else: \n",
    "        configs_list.append(annotations_configs[dataset])\n",
    "        image_dirs_list.append(image_dirs[dataset])\n",
    "    \n",
    "    print(image_dirs_list)\n",
    "    print(configs_list)\n",
    "    count = 0\n",
    "    for image_dir,annotations_config in zip(image_dirs_list, configs_list):\n",
    "        with open(annotations_config) as f:\n",
    "            configs = json.load(f)\n",
    "        label_vals = {'O' : 0, 'B-QUESTION' : 1, 'B-ANSWER' : 2, 'B-HEADER' : 3, 'I-ANSWER' : 4, 'I-QUESTION' : 5, 'I-HEADER' : 6}\n",
    "        label_vals_reversed = {val:key for (key,val) in label_vals.items()} \n",
    "        image_ids = []    \n",
    "        indices_to_keep = []\n",
    "        print(f'populating valid image indices for {annotations_config}')\n",
    "        for i,image_obj in enumerate(configs['images']):\n",
    "            image_ids.append(image_obj['id'])\n",
    "            \n",
    "            image_path = f'{image_dir}/{image_obj[\"file_name\"]}'\n",
    "            img_height = image_obj[\"height\"]\n",
    "            img_width = image_obj[\"width\"]\n",
    "        \n",
    "            combined_annotations = combine_annotations(image_id=image_obj['id'], configs=configs, tokenizer=tokenizer, model=model, image_dim={'width':img_width, 'height':img_height})\n",
    "            entities = form_entities(combined_annotations['token_cat_groups'], tokenizer, combined_annotations['input_ids'], combined_annotations['input_id_map'])\n",
    "            combined_annotations['entities'] = entities\n",
    "            _, relations = form_relations(entities, combined_annotations['input_ids'])\n",
    "            if len(relations['head']) > 0:\n",
    "                indices_to_keep.append(i)\n",
    "            \n",
    "        \n",
    "        print(f'Generating dataset {annotations_config}')\n",
    "        validation_percentage = int(train_test_split * len(indices_to_keep))\n",
    "    \n",
    "        validation_indices = random.choices(indices_to_keep, k= validation_percentage)\n",
    "        for i,image_obj in enumerate(configs['images']):\n",
    "            if i in indices_to_keep:\n",
    "                image_ids.append(image_obj['id'])    \n",
    "                image_path = f'{image_dir}/{image_obj[\"file_name\"]}'\n",
    "                img_height = image_obj[\"height\"]\n",
    "                img_width = image_obj[\"width\"]\n",
    "        \n",
    "                combined_annotations = combine_annotations(image_id=image_obj['id'], configs=configs, tokenizer=tokenizer, model=model, image_dim={'width':img_width, 'height':img_height})\n",
    "                entities = form_entities(combined_annotations['token_cat_groups'], tokenizer, combined_annotations['input_ids'], combined_annotations['input_id_map'])\n",
    "                combined_annotations['entities'] = entities\n",
    "                _, relations = form_relations(entities, combined_annotations['input_ids'])\n",
    "\n",
    "                image = cv2.imread(image_path)\n",
    "                image_resized = cv2.resize(image, (224,224))\n",
    "                bbox = combined_annotations['bboxes']\n",
    "                input_ids = combined_annotations['input_ids']\n",
    "                labels = combined_annotations['labels']\n",
    "        \n",
    "                if i not in validation_indices:\n",
    "                    cv2.imwrite(f'{dataset_dir}/train/{images_dir}/image_{indices_to_keep.index(i) + count}.jpeg', image)\n",
    "                    cv2.imwrite(f'{dataset_dir}/train/{images_resized_dir}/image_resized_{indices_to_keep.index(i)+count}.jpeg', image_resized)\n",
    "                    bbox_path = f'{dataset_dir}/train/{bbox_dir}/bbox_{indices_to_keep.index(i)+count}.p'\n",
    "                    labels_path = f'{dataset_dir}/train/{labels_dir}/labels_{indices_to_keep.index(i)+count}.p'\n",
    "                    entities_path = f'{dataset_dir}/train/{entities_dir}/entities_{indices_to_keep.index(i)+count}.p'\n",
    "                    input_ids_path = f'{dataset_dir}/train/{input_ids_dir}/input_ids_{indices_to_keep.index(i)+count}.p'\n",
    "                    realations_path = f'{dataset_dir}/train/{relations_dir}/relations_{indices_to_keep.index(i)+count}.p'\n",
    "            \n",
    "                    dump_pickle(file_name=bbox_path, collection=bbox)\n",
    "                    dump_pickle(file_name=labels_path, collection=labels)\n",
    "                    dump_pickle(file_name=entities_path, collection=entities)\n",
    "                    dump_pickle(file_name=input_ids_path, collection=input_ids)\n",
    "                    dump_pickle(file_name=realations_path, collection=relations)\n",
    "                else:\n",
    "                    cv2.imwrite(f'{dataset_dir}/validation/{images_dir}/image_{indices_to_keep.index(i)+count}.jpeg', image)\n",
    "                    cv2.imwrite(f'{dataset_dir}/validation/{images_resized_dir}/image_resized_{indices_to_keep.index(i)+count}.jpeg', image_resized)\n",
    "                    bbox_path = f'{dataset_dir}/validation/{bbox_dir}/bbox_{indices_to_keep.index(i)+count}.p'\n",
    "                    labels_path = f'{dataset_dir}/validation/{labels_dir}/labels_{indices_to_keep.index(i)+count}.p'\n",
    "                    entities_path = f'{dataset_dir}/validation/{entities_dir}/entities_{indices_to_keep.index(i)+count}.p'\n",
    "                    input_ids_path = f'{dataset_dir}/validation/{input_ids_dir}/input_ids_{indices_to_keep.index(i)+count}.p'\n",
    "                    realations_path = f'{dataset_dir}/validation/{relations_dir}/relations_{indices_to_keep.index(i)+count}.p'\n",
    "            \n",
    "                    dump_pickle(file_name=bbox_path, collection=bbox)\n",
    "                    dump_pickle(file_name=labels_path, collection=labels)\n",
    "                    dump_pickle(file_name=entities_path, collection=entities)\n",
    "                    dump_pickle(file_name=input_ids_path, collection=input_ids)\n",
    "                    dump_pickle(file_name=realations_path, collection=relations)\n",
    "        count += len(indices_to_keep)\n",
    "        print('-------')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populating valid image indices\n",
      "populating valid image indices\n",
      "populating valid image indices\n",
      "populating valid image indices\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root_dir = 'all_annotations/'\n",
    "annotations_dirs = {dir:f'{root_dir}/{dir}/annotations/instances_default.json' for dir in os.listdir(root_dir)}\n",
    "image_dirs = {dir:f'{root_dir}/{dir}/images' for dir in os.listdir(root_dir)}\n",
    "\n",
    "\n",
    "load_dataset_funsd(annotations_configs=annotations_dirs,image_dirs=image_dirs, tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 163, 322, 18, 300, 3, 298, 49, 227, 302, 16, 121, 252, 182, 46, 314]\n"
     ]
    }
   ],
   "source": [
    "split_data(train_test_split=0.05, dataset_dir='dataset_real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dir = 'tokenizer_added_tokens'\n",
    "model_dir = 'model_added_tokens'\n",
    "shutil.rmtree(tokenizer_dir)\n",
    "shutil.rmtree(model_dir)\n",
    "tokenizer.save_pretrained(tokenizer_dir)\n",
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing relations and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : equipment-nummer, Answer :  0010017431, start: 43, end: 44\n",
      "question : auftrags-nummer, Answer :  8109589315-0110, start: 45, end: 46\n",
      "question : kunden-nummer, Answer :  0055026449, start: 47, end: 48\n",
      "question : akten-nummer, Answer :  88206-981764, start: 49, end: 50\n",
      "question : leistungsort, Answer :  falkenried office 1strabenbahnring 3-520251hamburg, start: 51, end: 52\n",
      "question : fabrik-/herstell-nr, Answer :  5291846, start: 53, end: 36\n",
      "question : baujahr, Answer :  2003, start: 54, end: 0\n",
      "question : tragfhigkeit, Answer :  1000 kg, start: 55, end: 39\n",
      "question : hersteller, Answer :  konegmbh, start: 59, end: 37\n",
      "question : ausfhrende fa, Answer :  konegmbh, start: 60, end: 38\n",
      "question : prfergebnis, Answer :  ohnemnge, start: 61, end: 40\n",
      "question : prfort, Answer :  hamburg, start: 62, end: 42\n",
      "question : prfdatum, Answer :  19.03.2013, start: 63, end: 41\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "entities_path = f'dataset_real/train/entities/entities_{index}.p'\n",
    "with open(entities_path, 'rb') as f:\n",
    "    entities = pickle.load(f)\n",
    "\n",
    "relations_path = f'dataset_real/train/relations/relations_{index}.p'\n",
    "\n",
    "with open(relations_path, 'rb') as f:\n",
    "    relations = pickle.load(f)\n",
    "    \n",
    "input_ids_path = f'dataset_real/train/input_ids/input_ids_{index}.p'\n",
    "\n",
    "with open(input_ids_path, 'rb') as f:\n",
    "    input_ids = pickle.load(f)\n",
    "\n",
    "#print(relations)\n",
    "#print(entities)\n",
    "\n",
    "entities_names = []\n",
    "entities_with_boxes = []\n",
    "for i , (start, end) in enumerate(zip(entities['start'], entities['end'])):\n",
    "    #print(f'{i}, {tokenizer.decode(input_ids[start:end])} : {entities[\"label\"][i]} {entities[\"cat\"][i]}')\n",
    "    #if entities[\"label\"][i].lower() != 'other':\n",
    "    entities_names.append(tokenizer.decode(input_ids[start:end]))\n",
    "  #entities_names.append(tokenizer.decode(input_ids[start:end]))\n",
    "\n",
    "for i in range(len(relations['head'])):\n",
    "    print(f'question : {entities_names[relations[\"head\"][i]]}, Answer :  {entities_names[relations[\"tail\"][i]]}, start: {relations[\"head\"][i]}, end: {relations[\"tail\"][i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing bboxes without dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@765.774] global loadsave.cpp:244 findDecoder imread_('dataset/train/images/image_200.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/train/bbox/bbox_200.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataset/train/images/image_\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m.jpeg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m bbox_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdataset/train/bbox/bbox_\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m.p\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(bbox_path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     bbox \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munnormalize_box\u001b[39m(bbox, width, height):\n",
      "File \u001b[0;32m~/anaconda3/envs/py3_10/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/train/bbox/bbox_200.p'"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "image = cv2.imread(f'dataset/train/images/image_{index}.jpeg')\n",
    "bbox_path = f'dataset/train/bbox/bbox_{index}.p'\n",
    "with open(bbox_path, 'rb') as f:\n",
    "    bbox = pickle.load(f)\n",
    "\n",
    "def unnormalize_box(bbox, width, height):\n",
    "     return [\n",
    "         width * (bbox[0] / 1000),\n",
    "         height * (bbox[1] / 1000),\n",
    "         width * (bbox[2] / 1000),\n",
    "         height * (bbox[3] / 1000),\n",
    "     ]\n",
    "\n",
    "image = Image.fromarray(image)\n",
    "width, height = image.size\n",
    "print(width)\n",
    "print(height)\n",
    "\n",
    "true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(bbox)]\n",
    "image = np.array(image)\n",
    "for box in true_boxes:\n",
    "  box = [int(i) for i in box]\n",
    "  cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_creator import *\n",
    "\n",
    "configs_train = {\n",
    "    \"num_files\":30,\n",
    "    \"images_dir\" : \"images\",\n",
    "    \"images_resized_dir\" : \"images_resized\",\n",
    "    \"bbox_dir\":\"bbox\",\n",
    "    \"input_ids_dir\" : \"input_ids\",\n",
    "    \"labels_dir\" : \"labels\",\n",
    "    \"entities_dir\" : \"entities\",\n",
    "    \"relations_dir\" : \"relations\",\n",
    "    \"type\":\"train\",\n",
    "    \"clear_all_old_files\" : True,\n",
    "    \"clear_old_files_type\": [True, \"validation\"]\n",
    "}\n",
    "\n",
    "configs_validation = {\n",
    "    \"num_files\":30,\n",
    "    \"images_dir\" : \"images\",\n",
    "    \"images_resized_dir\" : \"images_resized\",\n",
    "    \"bbox_dir\":\"bbox\",\n",
    "    \"input_ids_dir\" : \"input_ids\",\n",
    "    \"labels_dir\" : \"labels\",\n",
    "    \"entities_dir\" : \"entities\",\n",
    "    \"relations_dir\" : \"relations\",\n",
    "    \"type\":\"validation\",\n",
    "    \"clear_all_old_files\" : True,\n",
    "    \"clear_old_files_type\": [True, \"validation\"]\n",
    "}\n",
    "\n",
    "configs_train = {key:val for key,val in configs_train.items() if key not in (\"num_files\", \"clear_all_old_files\", \"clear_old_files_type\")}\n",
    "configs_validation = {key:val for key,val in configs_validation.items() if key not in (\"num_files\", \"clear_all_old_files\", \"clear_old_files_type\")}\n",
    "\n",
    "dataset_train = Custom_Dataset(**configs_train)\n",
    "dataset_validation = Custom_Dataset(**configs_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_train[1]\n",
    "image = sample['original_image']\n",
    "image.shape\n",
    "bbox = sample['bbox']\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_box(bbox, width, height):\n",
    "     return [\n",
    "         width * (bbox[0] / 1000),\n",
    "         height * (bbox[1] / 1000),\n",
    "         width * (bbox[2] / 1000),\n",
    "         height * (bbox[3] / 1000),\n",
    "     ]\n",
    "\n",
    "#predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "#token_boxes = encoding.bbox.squeeze().tolist()\n",
    "\n",
    "image = Image.fromarray(image)\n",
    "width, height = image.size\n",
    "print(width)\n",
    "print(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(bbox)]\n",
    "#true_boxes = bbox\n",
    "print(true_boxes)\n",
    "def iob_to_label(label):\n",
    "    label = label[2:]\n",
    "    if not label:\n",
    "      return 'other'\n",
    "    return label\n",
    "\n",
    "label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}\n",
    "image = np.array(image)\n",
    "for box in true_boxes:\n",
    "  box = [int(i) for i in box]\n",
    "  cv2.rectangle(image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "    #draw.text((box[0]+10, box[1]-10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
