{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas_config = {\n",
    "    'start_x' :30,\n",
    "    'start_y' : 750,\n",
    "    'token_spacing' : 100,\n",
    "    'line_spacing' : 8,\n",
    "  'count' : 0,\n",
    "  'key_spacing' : 200,\n",
    "  'header_spacing' : 15,\n",
    "  'section_spacing' : 50,\n",
    "  'line_break' : 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from template1 import Template_Dekra\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen.canvas import Canvas as Canvas\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from reportlab.lib.colors import HexColor\n",
    "from reportlab.lib import colors\n",
    "import os\n",
    "from reportlab.lib.utils import ImageReader\n",
    "from reportlab.lib.units import inch\n",
    "import numpy as np\n",
    "from reportlab.lib.pagesizes import A4\n",
    "import random\n",
    "from transformers import LayoutLMv2ForRelationExtraction, AutoTokenizer\n",
    "from template1 import global_keys\n",
    "from template1 import * \n",
    "start_x = 30\n",
    "start_x_temp = start_x\n",
    "start_y = 750\n",
    "start_y_temp = start_y\n",
    "token_spacing = 100\n",
    "line_spacing = 8\n",
    "count = 0\n",
    "key_spacing = 200\n",
    "header_spacing = 15\n",
    "section_spacing = 30\n",
    "line_break = 20\n",
    "\n",
    "    #file_name='form.pdf'\n",
    "header = 'Zugelassene Überwachungsstelle Aufzüge'\n",
    "file_name = '20181119-32753-1891960176-100-421500.docx'\n",
    "report_name = 'form2.pdf'\n",
    "page_no = 'Seite 1 von 1'\n",
    "\n",
    "template1 = Template_Dekra(start_x = start_x,\n",
    "        start_y = start_y,\n",
    "        token_spacing = token_spacing,\n",
    "        line_spacing = line_spacing,\n",
    "        key_spacing = key_spacing,\n",
    "        header_spacing = header_spacing,\n",
    "        section_spacing = section_spacing,\n",
    "        line_break = line_break,\n",
    "        header = header,\n",
    "        file_name = file_name,\n",
    "        report_name=report_name,\n",
    "        page_no = page_no)\n",
    "\n",
    "\n",
    "global_keys = template1.draw_report(header=header, report_name='form2.pdf')\n",
    "model = LayoutLMv2ForRelationExtraction.from_pretrained(\"microsoft/layoutxlm-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/layoutxlm-base\")\n",
    "\n",
    "tokens, bboxes, image = get_ocr_data(image_path='form2.jpg')\n",
    "tokens, bboxes = preprocess_tokens(tokens=tokens, bboxes=bboxes)\n",
    "\n",
    "tokenizer, model = add_tokens_tokenizer(tokens = tokens, tokenizer = tokenizer, model = model)\n",
    "input_ids, bboxes, input_id_map = encode_tokens(tokens=tokens, bboxes=bboxes, tokenizer=tokenizer)\n",
    " \n",
    "key_vals_unified = unify_keys_vals(global_keys)\n",
    "#print(key_vals_unified)\n",
    "#token_group_key, token_group_val, token_group_others = template1.form_token_groups(unified_dict=key_vals_unified, tokens=tokens, bboxes=bboxes)\n",
    "key_set, val_set, token_map = form_token_groups(unified_dict=key_vals_unified, tokens=tokens, bboxes=bboxes)\n",
    "\n",
    "labels= label_input_ids(key_set=key_set, val_set=val_set, input_id_map=input_id_map, tokenizer=tokenizer, input_ids=input_ids)\n",
    "#print(val_set)\n",
    "#print(key_set)\n",
    "entities, entity_key_index_mapping, entity_key_index_mapping_reverse = form_entities(unified_dict=key_vals_unified,tokens = tokens,  bboxes = bboxes, input_ids=input_ids, input_id_map=input_id_map, tokenizer=tokenizer)\n",
    "\n",
    "#print(entity_key_index_mapping_reverse)\n",
    "relations = form_relations(entities=entities, unified_dict=key_vals_unified, key_set=key_set,  entity_key_index_mapping=entity_key_index_mapping, entity_key_index_mapping_reverse = entity_key_index_mapping_reverse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (320,25))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(key_set))\n",
    "print(len(val_set))\n",
    "#print(len(key_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(entities['start']))\n",
    "print(len(entities['end']))\n",
    "print(len(entities['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_names = []\n",
    "for i , (start, end) in enumerate(zip(entities['start'], entities['end'])):\n",
    "    print(f'{i}, {tokenizer.decode(input_ids[start:end])} : {entities[\"label\"][i]}')\n",
    "    entity_names.append(tokenizer.decode(input_ids[start:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relations)\n",
    "print(len(relations['head']))\n",
    "print(len(relations['tail']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(relations['head'])):\n",
    "    print(f'question : {entity_names[relations[\"head\"][i]]}, Answer :  {entity_names[relations[\"tail\"][i]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = entities['start'][32]\n",
    "end = entities['end'][32]\n",
    "\n",
    "print(start)\n",
    "print(end)\n",
    "print(f'asd{tokenizer.decode(input_ids[start])}asd')\n",
    "print(f'asd {tokenizer.decode(input_ids[end])} asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1= \"4,00\"\n",
    "token2= \"m/s\"\n",
    "id1 = input_id_map[token1]\n",
    "id2 = input_id_map[token2]\n",
    "index1=input_ids.index(id1)\n",
    "index2=input_ids.index(id2)\n",
    "\n",
    "tokenizer.decode(input_ids[index1:index2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([tokenizer.decode(id) for id in input_ids if 'uni' in tokenizer.decode(id)])\n",
    "print([id for id in input_ids if 'uni' in tokenizer.decode(id)])\n",
    "\n",
    "print([token for token in tokens if 'uni' in token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'form.pdf'\n",
    "file_name[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image = cv2.imread('form2.jpg')\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C://Program Files//Tesseract-OCR//tesseract.exe'\n",
    "\n",
    "results = pytesseract.image_to_data(image, output_type=Output.DICT, lang='deu')\n",
    "n_boxes = len(results['level'])\n",
    "#tokens = []\n",
    "#bboxes = []\n",
    "label_vals = {'O' : 0, 'B-QUESTION' : 1, 'B-ANSWER' : 2, 'B-HEADER' : 3, 'I-ANSWER' : 4, 'I-QUESTION' : 5, 'I-HEADER' : 6}\n",
    "label_vals_reversed = {0: 'O',  1: 'B-QUESTION',  2: 'B-ANSWER',  3: 'B-HEADER', 4 : 'I-ANSWER', 5 : 'I-QUESTION',  6 : 'I-HEADER'}\n",
    "#tokens = {'text' : [], 'bbox':[]}\n",
    "for i in range(len(bboxes)-1):\n",
    "    \n",
    "\t# text localization\n",
    "    text = label_vals_reversed[labels[i]]\n",
    "    \n",
    "    # with the text itself\n",
    "    x1,y1,x2,y2 = bboxes[i][0], bboxes[i][1], bboxes[i][2], bboxes[i][3] \n",
    "    text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.2, (0, 0, 255), 3)\n",
    "\n",
    "plt.figure(figsize = (320,25))\n",
    "plt.imshow(image)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = template1.encode_new_tokens(tokens = tokens, bboxes=bboxes, tokenizer=tokenizer)\n",
    "#len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids =template1.encode_tokens(tokens=tokens, bboxes=bboxes, tokenizer=tokenizer)\n",
    "all_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token = tokens[0] #, tokens[1]\n",
    "\n",
    "print(token)\n",
    "#token_ids = {tokenized_token:id for token in tokens for tokenized_token in tokenizer.tokenize(token) for id in input_ids if tokenizer.decode(id)== tokenized_token}\n",
    "print(tokenizer.tokenize(token))\n",
    "tokenized_token = tokenizer.tokenize(token)\n",
    "input_ids = tokenizer.encode(text = tokenized_token, boxes = [bboxes[0] for i in range(len(tokenized_token))], is_pretokenized=True, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "print(tokenizer.decode(input_ids[0:91151]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'O', 1: 'B-QUESTION', 2: 'B-ANSWER', 3: 'B-HEADER', 4: 'I-ANSWER', 5: 'I-QUESTION', 6: 'I-HEADER'}\n",
    "token = tokens[0] \n",
    "bboxe = bboxes[0]\n",
    "tokenized_token = tokenizer.tokenize(token)\n",
    "input_ids_token = [input_id_map[t] for t in tokenized_token]\n",
    "indices = [input_ids.index(i) for i in input_ids_token]\n",
    "print(indices)\n",
    "print(tokenized_token)\n",
    "print(input_ids_token)\n",
    "print([id2label[labels[i]] for i in indices ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokens[0] , tokens[1]\n",
    "\n",
    "print(token)\n",
    "#token_ids = {tokenized_token:id for token in tokens for tokenized_token in tokenizer.tokenize(token) for id in input_ids if tokenizer.decode(id)== tokenized_token}\n",
    "print(tokenizer.tokenize(token))\n",
    "tokenized_token = tokenizer.tokenize(token)\n",
    "input_ids = tokenizer.encode(text = tokenized_token, boxes = [bboxes[0] for i in range(len(tokenized_token))], is_pretokenized=True, add_special_tokens=False)\n",
    "input_ids2 = tokenizer.encode(text = tokenized_token, boxes = [bboxes[0] for i in range(len(tokenized_token))], is_pretokenized=True, add_special_tokens=False)\n",
    "print(input_ids)\n",
    "print(input_ids2)\n",
    "print(tokenizer.decode(input_ids[0:91151]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "editdistance.eval('banana', 'bahama')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake Address DE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from faker import Faker\n",
    "locales = ['de_DE']\n",
    "fake = Faker(locales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_openthesaurus import OpenThesaurusDb\n",
    "open_thesaurus = OpenThesaurusDb(host=\"localhost\", user=\"root\", passwd=\"root\", db_name=\"openthesaurus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the short version of synonyms as a list\n",
    "synonyms = open_thesaurus.get_synonyms(word=\"Prüfung\")\n",
    "\n",
    "# to get the long version of synonyms as a list\n",
    "synonyms_long = open_thesaurus.get_synonyms(word=\"Prüfgrundlage\", form=\"long\")\n",
    "print(synonyms)\n",
    "print(synonyms_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emplacement de l'objet\n",
      "Objektort\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def back_translate(text, target_language=\"fr\", source_language=\"de\"):\n",
    "    translator = Translator(service_urls=['translate.google.com'])\n",
    "    translation = translator.translate(text, dest=target_language).text\n",
    "    print(translation)\n",
    "    back_translation = translator.translate(translation, dest=source_language).text\n",
    "    return back_translation\n",
    "\n",
    "text = \"objektstandort\"\n",
    "translator = Translator(service_urls=['translate.google.com'])\n",
    "back_translated_text = back_translate(text, target_language=\"fr\", source_language=\"de\")\n",
    "print(back_translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], '', '')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# meaning of \"python\"\n",
    "from PyMultiDictionary import MultiDictionary\n",
    "dictionary = MultiDictionary()\n",
    "\n",
    "meaning = dictionary.meaning(\"de\",\"objektstandort\")\n",
    "print(meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictcc import Dict\n",
    "translator = Dict()\n",
    "result = translator.translate(\"Prüfgrundlage\", from_language=\"de\", to_language=\"en\")\n",
    "result.translation_tuples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from torch.TransferLearning import get_device\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "MODEL_NAME = 'tuner007/pegasus_paraphrase'\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('pegasus_english_tokenizer')\n",
    "model.save_pretrained('pegasus_english_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhij\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object location\n",
      "['physical_object', 'aim', 'objective', 'target']\n",
      "['placement', 'locating', 'position', 'positioning', 'emplacement', 'localization', 'localisation', 'locating', 'fix']\n"
     ]
    }
   ],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            if word != l.name():\n",
    "                synonyms.append(l.name())\n",
    "    return synonyms\n",
    "\n",
    "word = 'objektstandort'\n",
    "translate = translator.translate(word, dest='en').text\n",
    "print(translate)\n",
    "\n",
    "print(get_synonyms(translate.split(\" \")[0]))\n",
    "print(get_synonyms(translate.split(\" \")[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym 1: 1. Ortsbestimmung\n",
      "2. Standortbestimmung\n",
      "3. Lagebestimmung\n",
      "4. Positionierung\n",
      "5. Position\n",
      "6. Platzierung\n",
      "7. Lokalisierung\n",
      "8. Verortung\n",
      "9. Lokalisation\n",
      "10. Verlokalisierung\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_synonyms(word):\n",
    "    prompt = f\"generate 10 German synonyms for the word {word}\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    \n",
    "    synonyms = []\n",
    "    for choice in response.choices:\n",
    "        synonyms.append(choice.text.strip())\n",
    "    \n",
    "    return synonyms\n",
    "\n",
    "word = \"Objektstandort\"\n",
    "synonyms = generate_synonyms(word)\n",
    "for index, synonym in enumerate(synonyms):\n",
    "    print(f\"Synonym {index+1}: {synonym}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['objekt', '##standort']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-german-cased\")\n",
    "word = \"objektstandort\"\n",
    "print(tokenizer.tokenize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 188/188 [00:00<00:00, 101kB/s]\n",
      "Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:01<00:00, 3.20MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 150/150 [00:00<00:00, 74.6kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 5, 768)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "feature_extractor = pipeline(\"feature-extraction\", model=\"T-Systems-onsite/german-roberta-sentence-transformer-v2\", tokenizer=\"T-Systems-onsite/german-roberta-sentence-transformer-v2\")\n",
    "features = feature_extractor(word)\n",
    "np.array(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [1,2,3]\n",
    "type(list1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sache', 'Ding', 'Gegenstand', 'Teil', 'Gizmo', 'Etwas']\n",
      "['Sitz', 'Aufstellungsort']\n",
      "(1, 768)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28796/2781056060.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_synonyms_word1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_word1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_synonyms_word2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_w\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_synonyms_word1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from odenet import *\n",
    "#print(hypernyms_word('Objekt'))\n",
    "#print(hyponyms_word('Standort'))\n",
    "#print(synonyms_word('Standort'))\n",
    "\n",
    "word1 = 'Objekt'\n",
    "word2 = 'Standort'\n",
    "\n",
    "word1_synonyms = synonyms_word(word1)\n",
    "\n",
    "word1_synonyms = [word for word in word1_synonyms[0]]\n",
    "word2_synonyms = synonyms_word(word2)\n",
    "word2_synonyms = [word for word in word2_synonyms[0]]\n",
    "#word1_synonyms_dict = {word:[w for w in tokenizer.tokenize(word)] for word in word1_synonyms}\n",
    "#word2_synonyms_dict = {word:[w for w in tokenizer.tokenize(word)] for word in word2_synonyms}\n",
    "\n",
    "\n",
    "features_word1 = np.array(feature_extractor(word1)).mean(axis=1)\n",
    "features_word2 = np.array(feature_extractor(word2)).mean(axis=1)\n",
    "\n",
    "features_synonyms_word1 = [np.array(feature_extractor(word)).mean(axis=1) for word in word1_synonyms]\n",
    "features_synonyms_word2 = [np.array(feature_extractor(word)).mean(axis=1) for word in word2_synonyms]\n",
    "\n",
    "\n",
    "for i, f_w in enumerate(features_synonyms_word1):\n",
    "    print(f_w.shape)\n",
    "\n",
    "synonyms_by_combination = [word1+word2.lower() for word1 in word1_synonyms for word2 in word2_synonyms]\n",
    "len(synonyms_by_combination)\n",
    "synonyms_by_combination_features = [np.array(feature_extractor(word)).mean(axis=1) for word in synonyms_by_combination]\n",
    "synonyms_by_combination\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "#cosine_similarity_syn1 = {word:cosine(features_word1.reshape(1,-1)[0], f_w.reshape(1,-1)[0]) for word, f_w in zip(word1_synonyms, features_synonyms_word1)}\n",
    "feature_word = np.array(feature_extractor('objectstandort')).mean(axis = 1)\n",
    "cosine_similarity_syn = {word:cosine(feature_word.reshape(1,-1)[0], f_w.reshape(1,-1)[0]) for word, f_w in zip(synonyms_by_combination, synonyms_by_combination_features)}\n",
    "\n",
    "sorted(cosine_similarity_syn.items(), key = lambda x:x[1], reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sache', 'Ding', 'Gegenstand', 'Teil', 'Gizmo', 'Etwas', 'Größe', 'Symbol', 'Entität', 'Symbolfigur', 'Satzergänzung']\n",
      "['Sitz', 'Aufstellungsort', 'Lage', 'Position', 'Stelle', 'Punkt', 'Location', 'Ort', 'Fleck', 'Zweigniederlassung', 'Vertretung', 'Außenstelle', 'Geschäftsstelle', 'Tochterunternehmen', 'Zweiggeschäft', 'Filiale', 'Dependance', 'Niederlassung', 'Zweigstelle', 'Zweigbetrieb', 'Außenstandort', 'Kontor', 'Nebenstelle', 'Sitz', 'Aufenthaltsort', 'Wohnsitz', 'Meldeadresse', 'Wohnort', 'Aufenthalt', 'Wohnadresse', 'Sitz', 'Unternehmenssitz', 'Firmensitz', 'Betriebsstandort', 'Firmenstandort', 'Unternehmensstandort']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Satzergänzungfirmensitz', 0.8687272814870622),\n",
       " ('Satzergänzungunternehmenssitz', 0.8273779339745252),\n",
       " ('Gegenstandunternehmenssitz', 0.7625078587473786),\n",
       " ('Gegenstandfirmensitz', 0.7489279114951872)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from odenet import *\n",
    "#print(hypernyms_word('Objekt'))\n",
    "#print(hyponyms_word('Standort'))\n",
    "#print(synonyms_word('Standort'))\n",
    "\n",
    "word1 = 'Objekt'\n",
    "word2 = 'Standort'\n",
    "\n",
    "word1_synonyms = synonyms_word(word1)\n",
    "word1_synonyms = [word for list_ in word1_synonyms for word in list_]\n",
    "word2_synonyms = synonyms_word(word2)\n",
    "word2_synonyms = [word for list_ in word2_synonyms for word in list_]\n",
    "print(word1_synonyms)\n",
    "print(word2_synonyms)\n",
    "#word1_synonyms_dict = {word:[w for w in tokenizer.tokenize(word)] for word in word1_synonyms}\n",
    "#word2_synonyms_dict = {word:[w for w in tokenizer.tokenize(word)] for word in word2_synonyms}\n",
    "\n",
    "\n",
    "features_word1 = np.array(feature_extractor(word1)).mean(axis=1)\n",
    "features_word2 = np.array(feature_extractor(word2)).mean(axis=1)\n",
    "\n",
    "features_synonyms_word1 = [np.array(feature_extractor(word)).mean(axis=1) for word in word1_synonyms]\n",
    "features_synonyms_word2 = [np.array(feature_extractor(word)).mean(axis=1) for word in word2_synonyms]\n",
    "\n",
    "cosine_similarity_syn1 = {word:cosine(features_word1.reshape(1,-1)[0], f_w.reshape(1,-1)[0]) for word, f_w in zip(word1_synonyms, features_synonyms_word1)}\n",
    "cosine_similarity_syn2 = {word:cosine(features_word2.reshape(1,-1)[0], f_w.reshape(1,-1)[0]) for word, f_w in zip(word2_synonyms, features_synonyms_word2)}\n",
    "\n",
    "\n",
    "\n",
    "cosine_similarity_syn1_sorted = sorted(cosine_similarity_syn1.items(), key= lambda x:x[1], reverse = True)\n",
    "cosine_similarity_syn1_sorted = [cosine_similarity_syn1_sorted[i][0] for i in range(len(cosine_similarity_syn1_sorted)) if i<2]\n",
    "#print(cosine_similarity_syn1_sorted)\n",
    "\n",
    "cosine_similarity_syn2_sorted = sorted(cosine_similarity_syn2.items(), key= lambda x:x[1], reverse = True)\n",
    "\n",
    "cosine_similarity_syn2_sorted = [cosine_similarity_syn2_sorted[i][0] for i in range(len(cosine_similarity_syn2_sorted)) if i<2]\n",
    "\n",
    "word1_synonyms = word1_synonyms[:2]\n",
    "word2_synonyms = word2_synonyms[:2]\n",
    "\n",
    "synonyms_by_combination = [word1+word2.lower() for word1 in cosine_similarity_syn1_sorted for word2 in cosine_similarity_syn2_sorted]\n",
    "synonyms_by_combination_features = [np.array(feature_extractor(word)).mean(axis=1) for word in synonyms_by_combination]\n",
    "synonyms_by_combination\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "#cosine_similarity_syn1 = {word:cosine(features_word1.reshape(1,-1)[0], f_w.reshape(1,-1)[0]) for word, f_w in zip(word1_synonyms, features_synonyms_word1)}\n",
    "feature_word = np.array(feature_extractor('objectstandort')).mean(axis = 1)\n",
    "cosine_similarity_syn = {word:cosine(feature_word.reshape(1,-1)[0], f_w.reshape(1,-1)[0]) for word, f_w in zip(synonyms_by_combination, synonyms_by_combination_features)}\n",
    "\n",
    "sorted(cosine_similarity_syn.items(), key = lambda x:x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
